# Enhanced Keyword Gap Analysis v2.0 - Technical Documentation

## üìã Table of Contents
1. [Architecture Overview](#architecture-overview)
2. [Data Models](#data-models)
3. [Core Components](#core-components)
4. [Analysis Engine](#analysis-engine)
5. [Web Interface](#web-interface)
6. [Database Schema](#database-schema)
7. [API Endpoints](#api-endpoints)
8. [Configuration](#configuration)
9. [Deployment](#deployment)
10. [Testing](#testing)
11. [Performance](#performance)
12. [Security](#security)

---

## üèóÔ∏è Architecture Overview

### System Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    Frontend Layer                           ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  keyword_gap_v2.html  ‚îÇ  keyword_gap_result_v2.html        ‚îÇ
‚îÇ  (Input Form)         ‚îÇ  (Results Dashboard)               ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              ‚îÇ
                              ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    Web Layer                               ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  keyword_gap_v2.py    ‚îÇ  Flask Blueprint                   ‚îÇ
‚îÇ  (Route Handler)      ‚îÇ  (Request Processing)              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              ‚îÇ
                              ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    Business Logic Layer                    ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  EnhancedKeywordGapAnalyzer ‚îÇ  IntentAnalyzer              ‚îÇ
‚îÇ  (Main Analysis Engine)     ‚îÇ  (Intent Classification)     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              ‚îÇ
                              ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    Data Layer                              ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  models.py            ‚îÇ  Data Classes & Enums             ‚îÇ
‚îÇ  (Data Models)         ‚îÇ  (Business Logic)                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Component Dependencies

```
EnhancedKeywordGapAnalyzer
‚îú‚îÄ‚îÄ IntentAnalyzer
‚îú‚îÄ‚îÄ BusinessContext
‚îú‚îÄ‚îÄ SearchQuery
‚îú‚îÄ‚îÄ KeywordGapOpportunity
‚îî‚îÄ‚îÄ KeywordGapAnalysisResult

keyword_gap_v2.py
‚îú‚îÄ‚îÄ EnhancedKeywordGapAnalyzer
‚îú‚îÄ‚îÄ BusinessContext
‚îî‚îÄ‚îÄ Flask (render_template)

Templates
‚îú‚îÄ‚îÄ keyword_gap_v2.html
‚îî‚îÄ‚îÄ keyword_gap_result_v2.html
```

---

## üìä Data Models

### Core Enums

```python
class SearchIntent(Enum):
    INFORMATIONAL = "informational"
    TRANSACTIONAL = "transactional"
    LOCAL = "local"
    COMPARISON = "comparison"
    NAVIGATIONAL = "navigational"

class KeywordDifficulty(Enum):
    EASY = "easy"
    MEDIUM = "medium"
    HARD = "hard"
    EXPERT = "expert"

class ContentType(Enum):
    ARTICLE = "article"
    SERVICE_PAGE = "service_page"
    PRODUCT_PAGE = "product_page"
    LANDING_PAGE = "landing_page"
    FAQ = "faq"
    COMPARISON = "comparison"
    LOCAL = "local"

class SERPFeature(Enum):
    VIDEO = "video"
    IMAGES = "images"
    MAPS = "maps"
    FAQ = "faq"
    ADS = "ads"
    NEWS = "news"
    SHOPPING = "shopping"
```

### Business Context Model

```python
@dataclass
class BusinessContext:
    """Business context for context-aware analysis"""
    industry: str
    niche: str
    services: List[str]
    products: List[str]
    target_locations: List[str]
    brand_keywords: List[str]
    excluded_keywords: List[str]
    
    def __post_init__(self):
        """Validate and normalize business context"""
        self.industry = self.industry.lower().strip()
        self.niche = self.niche.lower().strip()
        self.services = [s.strip() for s in self.services if s.strip()]
        self.products = [p.strip() for p in self.products if p.strip()]
        self.target_locations = [l.strip() for l in self.target_locations if l.strip()]
        self.brand_keywords = [k.strip() for k in self.brand_keywords if k.strip()]
        self.excluded_keywords = [k.strip() for k in self.excluded_keywords if k.strip()]
```

### Search Query Model

```python
@dataclass
class SearchQuery:
    """Represents a keyword as a demand unit"""
    query_text: str
    search_intent: SearchIntent
    search_volume_estimate: int
    difficulty: KeywordDifficulty
    relevance_to_business: float
    serp_features: List[SERPFeature]
    recommended_content_type: ContentType
    ngram_size: int
    is_long_tail: bool
    found_on_competitors: bool
    competitor_avg_position: float
    tf_score: float
    idf_score: float
    tfidf_score: float
    ctr_potential: float
    estimated_traffic_value: float
    
    def __post_init__(self):
        """Calculate derived fields"""
        self.is_long_tail = self.ngram_size >= 3
        self.estimated_traffic_value = self.search_volume_estimate * self.ctr_potential * self.relevance_to_business
```

### Keyword Gap Opportunity Model

```python
@dataclass
class KeywordGapOpportunity:
    """Represents a keyword gap opportunity"""
    query: SearchQuery
    opportunity_score: float
    volume_score: float
    relevance_score: float
    difficulty_score: float
    intent_match_score: float
    competition_score: float
    priority_tier: str
    effort_estimate_hours: float
    estimated_monthly_traffic: int
    recommended_actions: List[str]
    content_type_needed: ContentType
    strategic_importance: str
    competitor_analysis: Dict
    
    def __post_init__(self):
        """Calculate derived metrics"""
        self.estimated_monthly_traffic = int(
            self.query.search_volume_estimate * 
            self.relevance_score * 
            self.ctr_potential
        )
```

### Analysis Result Model

```python
@dataclass
class KeywordGapAnalysisResult:
    """Complete analysis results"""
    total_opportunities: int
    estimated_traffic_potential: int
    quick_wins: List[KeywordGapOpportunity]
    high_priority: List[KeywordGapOpportunity]
    informational_gaps: List[KeywordGapOpportunity]
    transactional_gaps: List[KeywordGapOpportunity]
    local_gaps: List[KeywordGapOpportunity]
    strategic_recommendations: List[str]
    content_calendar_suggestions: List[Dict]
    summary_metrics: Dict
    analysis_timestamp: datetime
    business_context: BusinessContext
    
    def get_priority_matrix(self) -> Dict:
        """Generate priority matrix data"""
        return {
            'quick_wins': len(self.quick_wins),
            'high_priority': len(self.high_priority),
            'informational': len(self.informational_gaps),
            'transactional': len(self.transactional_gaps),
            'local': len(self.local_gaps)
        }
```

---

## ‚öôÔ∏è Core Components

### Enhanced Keyword Gap Analyzer

```python
class EnhancedKeywordGapAnalyzer:
    """Main analysis engine for keyword gap analysis"""
    
    def __init__(self, business_context: BusinessContext = None):
        self.business_context = business_context
        self.intent_analyzer = IntentAnalyzer()
        self.stop_words = self._load_stop_words()
    
    def analyze_keyword_gap(self, own_website: str, competitors: List[str], 
                          business_context: BusinessContext) -> KeywordGapAnalysisResult:
        """Main analysis method"""
        # 1. Extract content from websites
        own_content = self._extract_website_content(own_website)
        competitor_contents = [self._extract_website_content(url) for url in competitors]
        
        # 2. Extract N-grams from all content
        own_ngrams = self._extract_ngrams_from_content(own_content)
        competitor_ngrams = []
        for content in competitor_contents:
            competitor_ngrams.extend(self._extract_ngrams_from_content(content))
        
        # 3. Find gaps
        gaps = self._identify_keyword_gaps(own_ngrams, competitor_ngrams)
        
        # 4. Analyze each gap
        opportunities = []
        for gap in gaps:
            opportunity = self._analyze_gap_opportunity(gap, business_context)
            opportunities.append(opportunity)
        
        # 5. Classify and prioritize
        classified_opportunities = self._classify_opportunities(opportunities)
        
        # 6. Generate strategic recommendations
        recommendations = self._generate_strategic_recommendations(classified_opportunities)
        
        return KeywordGapAnalysisResult(
            total_opportunities=len(opportunities),
            estimated_traffic_potential=sum(opp.estimated_monthly_traffic for opp in opportunities),
            quick_wins=classified_opportunities['quick_wins'],
            high_priority=classified_opportunities['high_priority'],
            informational_gaps=classified_opportunities['informational'],
            transactional_gaps=classified_opportunities['transactional'],
            local_gaps=classified_opportunities['local'],
            strategic_recommendations=recommendations,
            content_calendar_suggestions=self._generate_content_calendar(classified_opportunities),
            summary_metrics=self._calculate_summary_metrics(opportunities),
            analysis_timestamp=datetime.now(),
            business_context=business_context
        )
```

### Intent Analyzer

```python
class IntentAnalyzer:
    """Analyzes search intent from content"""
    
    def __init__(self):
        self.intent_signals = self._load_intent_signals()
    
    def detect_intent(self, title: str, url: str, content: str = "", 
                     h1_tags: List[str] = None) -> Tuple[SearchIntent, float]:
        """Detect search intent with confidence score"""
        # Combine all text sources
        text_to_analyze = f"{title} {url} {content[:1000]} {' '.join(h1_tags or [])}"
        text_lower = text_to_analyze.lower()
        
        # Score each intent type
        scores = {}
        for intent, signals in self.intent_signals.items():
            score = 0.0
            
            # Keyword matching
            keyword_matches = sum(1 for kw in signals['keywords'] if kw in text_lower)
            score += keyword_matches * signals['weight']
            
            # URL pattern matching
            url_matches = sum(1 for pattern in signals['url_patterns'] 
                            if re.search(pattern, url, re.IGNORECASE))
            score += url_matches * 2.0 * signals['weight']
            
            # Title pattern matching
            title_matches = sum(1 for pattern in signals['title_patterns'] 
                             if pattern in title.lower())
            score += title_matches * 1.5 * signals['weight']
            
            scores[intent] = score
        
        # Find dominant intent
        if not any(scores.values()):
            return SearchIntent.INFORMATIONAL, 0.5
        
        max_intent = max(scores.items(), key=lambda x: x[1])
        intent = max_intent[0]
        raw_score = max_intent[1]
        
        # Calculate confidence
        total_score = sum(scores.values())
        confidence = min(raw_score / total_score if total_score > 0 else 0.5, 1.0)
        
        return intent, confidence
```

---

## üåê Web Interface

### Flask Blueprint Structure

```python
# app/web/keyword_gap_v2.py
keyword_gap_v2_bp = Blueprint("keyword_gap_v2", __name__, url_prefix="/keyword-gap-v2")

@keyword_gap_v2_bp.route("/", methods=["GET", "POST"])
@login_required
def keyword_gap_v2_index():
    """Enhanced Keyword Gap Analysis page"""
    if request.method == "POST":
        # Process form data
        own_website = request.form.get("own_website", "").strip()
        competitors = [c.strip() for c in request.form.getlist("competitors[]") if c.strip()]
        
        # Build business context
        business_context = BusinessContext(
            industry=request.form.get("industry", "").strip(),
            niche=request.form.get("niche", "").strip(),
            services=[s.strip() for s in request.form.get("services", "").split('\n') if s.strip()],
            products=[],  # Could be added later
            target_locations=[l.strip() for l in request.form.get("target_locations", "").split(',') if l.strip()],
            brand_keywords=[],  # Could be extracted from URL
            excluded_keywords=[]
        )
        
        # Perform analysis
        analyzer = EnhancedKeywordGapAnalyzer(business_context=business_context)
        result = analyzer.analyze_keyword_gap(own_website, competitors, business_context)
        
        # Render results
        return render_template("keyword_gap_result_v2.html", 
                             result=result, 
                             business_context=business_context)
    
    return render_template("keyword_gap_v2.html")
```

### Template Structure

#### Input Form (keyword_gap_v2.html)
```html
<form method="POST" class="analysis-form">
    <!-- Website URLs Section -->
    <div class="form-section">
        <h3>Website URLs</h3>
        <div class="form-group">
            <label for="own_website">Your Website URL</label>
            <input type="url" id="own_website" name="own_website" required>
        </div>
        <div class="competitors-section">
            <div class="competitor-input">
                <input type="url" name="competitors[]" placeholder="Competitor URL">
                <button type="button" class="remove-competitor">Remove</button>
            </div>
            <button type="button" class="add-competitor">+ Add Competitor</button>
        </div>
    </div>
    
    <!-- Business Context Section -->
    <div class="form-section">
        <h3>Business Context <span class="badge new">NEW</span></h3>
        <div class="form-group">
            <label for="industry">Industry</label>
            <select id="industry" name="industry">
                <option value="">Select Industry</option>
                <option value="medical">Medical/Healthcare</option>
                <option value="ecommerce">E-commerce</option>
                <option value="services">Professional Services</option>
                <!-- More options -->
            </select>
        </div>
        <!-- More business context fields -->
    </div>
</form>
```

#### Results Dashboard (keyword_gap_result_v2.html)
```html
<!-- Summary Cards -->
<div class="summary-grid">
    <div class="summary-card">
        <div class="card-title">Total Opportunities</div>
        <div class="card-value">{{ result.total_opportunities }}</div>
    </div>
    <!-- More summary cards -->
</div>

<!-- Priority Matrix -->
<div class="priority-matrix">
    <h3>Priority Matrix</h3>
    <div class="matrix-chart">
        <!-- Visual representation of opportunities -->
    </div>
</div>

<!-- Opportunity Categories -->
<div class="opportunities-tabs">
    <div class="tab-buttons">
        <button class="tab-btn active" data-tab="quick-wins">Quick Wins</button>
        <button class="tab-btn" data-tab="high-priority">High Priority</button>
        <!-- More tabs -->
    </div>
    
    <div class="tab-content">
        <div id="quick-wins" class="tab-pane active">
            {{ render_opportunities(result.quick_wins[:10]) }}
        </div>
        <!-- More tab panes -->
    </div>
</div>
```

---

## üóÑÔ∏è Database Schema

### Analysis Results Storage

```sql
-- Analysis sessions
CREATE TABLE analysis_sessions (
    id SERIAL PRIMARY KEY,
    user_id INTEGER,
    own_website VARCHAR(500),
    competitors TEXT[], -- Array of competitor URLs
    business_context JSONB, -- Business context data
    created_at TIMESTAMP DEFAULT NOW(),
    completed_at TIMESTAMP,
    status VARCHAR(50) DEFAULT 'pending'
);

-- Keyword opportunities
CREATE TABLE keyword_opportunities (
    id SERIAL PRIMARY KEY,
    session_id INTEGER REFERENCES analysis_sessions(id),
    query_text VARCHAR(500),
    search_intent VARCHAR(50),
    difficulty VARCHAR(20),
    relevance_score DECIMAL(5,2),
    opportunity_score DECIMAL(5,2),
    priority_tier VARCHAR(50),
    estimated_traffic INTEGER,
    effort_hours DECIMAL(5,2),
    content_type VARCHAR(50),
    recommended_actions TEXT[],
    created_at TIMESTAMP DEFAULT NOW()
);

-- Strategic recommendations
CREATE TABLE strategic_recommendations (
    id SERIAL PRIMARY KEY,
    session_id INTEGER REFERENCES analysis_sessions(id),
    recommendation_type VARCHAR(100),
    title VARCHAR(500),
    description TEXT,
    priority VARCHAR(20),
    created_at TIMESTAMP DEFAULT NOW()
);
```

### Indexes for Performance

```sql
-- Performance indexes
CREATE INDEX idx_analysis_sessions_user_id ON analysis_sessions(user_id);
CREATE INDEX idx_analysis_sessions_created_at ON analysis_sessions(created_at);
CREATE INDEX idx_keyword_opportunities_session_id ON keyword_opportunities(session_id);
CREATE INDEX idx_keyword_opportunities_priority ON keyword_opportunities(priority_tier);
CREATE INDEX idx_keyword_opportunities_score ON keyword_opportunities(opportunity_score DESC);
```

---

## üîå API Endpoints

### REST API Endpoints

```python
# app/web/api/keyword_gap_v2.py

@api_bp.route('/api/v2/keyword-gap/analyze', methods=['POST'])
@login_required
def analyze_keyword_gap():
    """API endpoint for keyword gap analysis"""
    data = request.get_json()
    
    # Validate input
    if not data.get('own_website'):
        return jsonify({'error': 'own_website is required'}), 400
    
    if not data.get('competitors'):
        return jsonify({'error': 'competitors are required'}), 400
    
    # Build business context
    business_context = BusinessContext(
        industry=data.get('industry', ''),
        niche=data.get('niche', ''),
        services=data.get('services', []),
        products=data.get('products', []),
        target_locations=data.get('target_locations', []),
        brand_keywords=data.get('brand_keywords', []),
        excluded_keywords=data.get('excluded_keywords', [])
    )
    
    # Perform analysis
    analyzer = EnhancedKeywordGapAnalyzer(business_context=business_context)
    result = analyzer.analyze_keyword_gap(
        data['own_website'],
        data['competitors'],
        business_context
    )
    
    # Return results
    return jsonify({
        'success': True,
        'result': {
            'total_opportunities': result.total_opportunities,
            'estimated_traffic': result.estimated_traffic_potential,
            'quick_wins': len(result.quick_wins),
            'high_priority': len(result.high_priority),
            'analysis_timestamp': result.analysis_timestamp.isoformat()
        }
    })

@api_bp.route('/api/v2/keyword-gap/opportunities/<int:session_id>')
@login_required
def get_opportunities(session_id):
    """Get opportunities for a specific analysis session"""
    # Implementation for retrieving opportunities
    pass

@api_bp.route('/api/v2/keyword-gap/recommendations/<int:session_id>')
@login_required
def get_recommendations(session_id):
    """Get strategic recommendations for a session"""
    # Implementation for retrieving recommendations
    pass
```

---

## ‚öôÔ∏è Configuration

### Environment Variables

```bash
# app/config.py

class Config:
    # Database
    DATABASE_URL = os.getenv('DATABASE_URL', 'sqlite:///seoanalyze.db')
    
    # Analysis Settings
    MAX_COMPETITORS = int(os.getenv('MAX_COMPETITORS', '10'))
    MAX_NGRAM_LENGTH = int(os.getenv('MAX_NGRAM_LENGTH', '5'))
    MIN_RELEVANCE_SCORE = float(os.getenv('MIN_RELEVANCE_SCORE', '0.3'))
    
    # Performance
    ANALYSIS_TIMEOUT = int(os.getenv('ANALYSIS_TIMEOUT', '300'))  # 5 minutes
    MAX_CONTENT_LENGTH = int(os.getenv('MAX_CONTENT_LENGTH', '10000'))
    
    # Business Context
    SUPPORTED_INDUSTRIES = [
        'medical', 'ecommerce', 'services', 'local', 'technology', 'education'
    ]
    
    # Intent Analysis
    INTENT_CONFIDENCE_THRESHOLD = 0.6
    INTENT_SIGNALS_FILE = 'app/core/data/intent_signals.json'
    
    # Stop Words
    STOP_WORDS_PERSIAN = 'app/core/data/stop_words_persian.txt'
    STOP_WORDS_ENGLISH = 'app/core/data/stop_words_english.txt'
```

### Analysis Parameters

```python
# app/core/config/analysis_config.py

ANALYSIS_CONFIG = {
    'ngram_extraction': {
        'min_length': 1,
        'max_length': 5,
        'min_frequency': 2,
        'exclude_stop_words': True
    },
    'intent_analysis': {
        'confidence_threshold': 0.6,
        'use_machine_learning': False,  # Future enhancement
        'fallback_intent': 'informational'
    },
    'relevance_scoring': {
        'industry_weight': 0.3,
        'services_weight': 0.4,
        'location_weight': 0.2,
        'brand_weight': 0.1
    },
    'opportunity_scoring': {
        'volume_weight': 0.3,
        'relevance_weight': 0.4,
        'difficulty_weight': 0.2,
        'intent_weight': 0.1
    }
}
```

---

## üöÄ Deployment

### Production Deployment

```bash
# 1. Install dependencies
pip install -r requirements.txt

# 2. Set environment variables
export FLASK_ENV=production
export DATABASE_URL=postgresql://user:pass@localhost/seoanalyze

# 3. Initialize database
flask db upgrade

# 4. Configure Gunicorn
gunicorn -w 3 -b 127.0.0.1:5000 --timeout 120 app.web.app:app

# 5. Configure Nginx
server {
    listen 80;
    server_name yourdomain.com;
    
    location / {
        proxy_pass http://127.0.0.1:5000;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
    }
}
```

### Docker Deployment

```dockerfile
# Dockerfile
FROM python:3.12-slim

WORKDIR /app
COPY requirements.txt .
RUN pip install -r requirements.txt

COPY . .
EXPOSE 5000

CMD ["gunicorn", "-w", "3", "-b", "0.0.0.0:5000", "--timeout", "120", "app.web.app:app"]
```

```yaml
# docker-compose.yml
version: '3.8'
services:
  web:
    build: .
    ports:
      - "5000:5000"
    environment:
      - DATABASE_URL=postgresql://postgres:password@db:5432/seoanalyze
    depends_on:
      - db
  
  db:
    image: postgres:15
    environment:
      - POSTGRES_DB=seoanalyze
      - POSTGRES_PASSWORD=password
    volumes:
      - postgres_data:/var/lib/postgresql/data

volumes:
  postgres_data:
```

---

## üß™ Testing

### Unit Tests

```python
# tests/test_enhanced_analyzer.py
import unittest
from app.core.enhanced_keyword_gap_analyzer import EnhancedKeywordGapAnalyzer
from app.core.models import BusinessContext

class TestEnhancedAnalyzer(unittest.TestCase):
    
    def setUp(self):
        self.business_context = BusinessContext(
            industry="medical",
            niche="cosmetic surgery",
            services=["laser hair removal"],
            target_locations=["Tehran"]
        )
        self.analyzer = EnhancedKeywordGapAnalyzer(self.business_context)
    
    def test_ngram_extraction(self):
        content = "ŸÑ€åÿ≤ÿ± ŸÖŸàŸáÿß€å ÿ≤ÿßÿ¶ÿØ ÿØÿ± ÿ™Ÿáÿ±ÿßŸÜ ÿ®Ÿáÿ™ÿ±€åŸÜ ⁄©ŸÑ€åŸÜ€å⁄©"
        ngrams = self.analyzer.extract_ngrams(content, max_length=3)
        
        self.assertIn("ŸÑ€åÿ≤ÿ± ŸÖŸàŸáÿß€å ÿ≤ÿßÿ¶ÿØ", ngrams)
        self.assertIn("ŸÖŸàŸáÿß€å ÿ≤ÿßÿ¶ÿØ ÿ™Ÿáÿ±ÿßŸÜ", ngrams)
    
    def test_relevance_scoring(self):
        keyword = "ŸÑ€åÿ≤ÿ± ŸÖŸàŸáÿß€å ÿ≤ÿßÿ¶ÿØ ÿ™Ÿáÿ±ÿßŸÜ"
        score = self.analyzer.calculate_relevance_score(keyword, self.business_context)
        
        self.assertGreater(score, 0.5)  # Should be highly relevant
    
    def test_intent_detection(self):
        title = "ŸÇ€åŸÖÿ™ ŸÑ€åÿ≤ÿ± ŸÖŸàŸáÿß€å ÿ≤ÿßÿ¶ÿØ ÿØÿ± ÿ™Ÿáÿ±ÿßŸÜ"
        url = "https://example.com/laser-price"
        
        intent, confidence = self.analyzer.intent_analyzer.detect_intent(title, url)
        
        self.assertEqual(intent.value, "transactional")
        self.assertGreater(confidence, 0.6)
```

### Integration Tests

```python
# tests/test_integration.py
import unittest
from app.web.keyword_gap_v2 import keyword_gap_v2_bp
from app.core.models import BusinessContext

class TestKeywordGapIntegration(unittest.TestCase):
    
    def test_full_analysis_workflow(self):
        # Test complete analysis workflow
        business_context = BusinessContext(
            industry="medical",
            niche="cosmetic surgery",
            services=["laser hair removal"],
            target_locations=["Tehran"]
        )
        
        analyzer = EnhancedKeywordGapAnalyzer(business_context)
        result = analyzer.analyze_keyword_gap(
            "https://example.com",
            ["https://competitor1.com"],
            business_context
        )
        
        self.assertGreater(result.total_opportunities, 0)
        self.assertGreater(len(result.quick_wins), 0)
        self.assertIsNotNone(result.strategic_recommendations)
```

### Performance Tests

```python
# tests/test_performance.py
import time
import unittest
from app.core.enhanced_keyword_gap_analyzer import EnhancedKeywordGapAnalyzer

class TestPerformance(unittest.TestCase):
    
    def test_analysis_performance(self):
        """Test that analysis completes within reasonable time"""
        analyzer = EnhancedKeywordGapAnalyzer()
        
        start_time = time.time()
        result = analyzer.analyze_keyword_gap(
            "https://example.com",
            ["https://competitor1.com", "https://competitor2.com"],
            BusinessContext(industry="medical", niche="surgery")
        )
        end_time = time.time()
        
        analysis_time = end_time - start_time
        self.assertLess(analysis_time, 60)  # Should complete within 60 seconds
```

---

## ‚ö° Performance

### Optimization Strategies

#### 1. **Caching**
```python
# app/core/cache.py
from functools import lru_cache
import redis

redis_client = redis.Redis(host='localhost', port=6379, db=0)

@lru_cache(maxsize=1000)
def cached_content_extraction(url: str) -> Dict:
    """Cache website content extraction"""
    # Implementation
    pass

def cache_analysis_result(session_id: str, result: Dict):
    """Cache analysis results"""
    redis_client.setex(f"analysis:{session_id}", 3600, json.dumps(result))
```

#### 2. **Async Processing**
```python
# app/core/tasks.py
from celery import Celery

celery_app = Celery('seoanalyze')

@celery_app.task
def analyze_keyword_gap_async(own_website: str, competitors: List[str], 
                            business_context: Dict) -> str:
    """Async keyword gap analysis"""
    analyzer = EnhancedKeywordGapAnalyzer()
    result = analyzer.analyze_keyword_gap(own_website, competitors, business_context)
    
    # Save result to database
    session_id = save_analysis_result(result)
    return session_id
```

#### 3. **Database Optimization**
```sql
-- Optimized queries
CREATE INDEX CONCURRENTLY idx_opportunities_score_priority 
ON keyword_opportunities(opportunity_score DESC, priority_tier);

-- Partitioning for large datasets
CREATE TABLE keyword_opportunities_2025 PARTITION OF keyword_opportunities
FOR VALUES FROM ('2025-01-01') TO ('2026-01-01');
```

### Performance Monitoring

```python
# app/core/monitoring.py
import time
from functools import wraps

def monitor_performance(func):
    """Decorator to monitor function performance"""
    @wraps(func)
    def wrapper(*args, **kwargs):
        start_time = time.time()
        result = func(*args, **kwargs)
        end_time = time.time()
        
        # Log performance metrics
        logger.info(f"{func.__name__} took {end_time - start_time:.2f} seconds")
        return result
    return wrapper

# Usage
@monitor_performance
def analyze_keyword_gap(self, own_website, competitors, business_context):
    # Analysis implementation
    pass
```

---

## üîí Security

### Input Validation

```python
# app/core/validation.py
import re
from urllib.parse import urlparse

def validate_website_url(url: str) -> bool:
    """Validate website URL format"""
    try:
        result = urlparse(url)
        return all([result.scheme, result.netloc])
    except:
        return False

def sanitize_business_context(context: BusinessContext) -> BusinessContext:
    """Sanitize business context input"""
    # Remove potentially harmful content
    context.industry = re.sub(r'[<>"\']', '', context.industry)
    context.niche = re.sub(r'[<>"\']', '', context.niche)
    
    # Limit list lengths
    context.services = context.services[:50]  # Max 50 services
    context.target_locations = context.target_locations[:20]  # Max 20 locations
    
    return context
```

### Rate Limiting

```python
# app/core/rate_limiting.py
from flask_limiter import Limiter
from flask_limiter.util import get_remote_address

limiter = Limiter(
    app,
    key_func=get_remote_address,
    default_limits=["100 per hour"]
)

@limiter.limit("5 per minute")
def analyze_keyword_gap():
    """Rate limited analysis endpoint"""
    pass
```

### Data Privacy

```python
# app/core/privacy.py
import hashlib

def anonymize_user_data(user_id: int, data: str) -> str:
    """Anonymize user data for analysis"""
    salt = f"user_{user_id}_salt"
    return hashlib.sha256(f"{data}{salt}".encode()).hexdigest()

def clean_sensitive_data(analysis_result: Dict) -> Dict:
    """Remove sensitive data from analysis results"""
    # Remove any personally identifiable information
    cleaned_result = analysis_result.copy()
    
    # Remove sensitive fields
    sensitive_fields = ['user_ip', 'user_agent', 'session_id']
    for field in sensitive_fields:
        cleaned_result.pop(field, None)
    
    return cleaned_result
```

---

## üìä Monitoring & Logging

### Application Logging

```python
# app/core/logging.py
import logging
from logging.handlers import RotatingFileHandler

def setup_logging(app):
    """Setup application logging"""
    if not app.debug:
        file_handler = RotatingFileHandler('logs/seoanalyze.log', maxBytes=10240, backupCount=10)
        file_handler.setFormatter(logging.Formatter(
            '%(asctime)s %(levelname)s: %(message)s [in %(pathname)s:%(lineno)d]'
        ))
        file_handler.setLevel(logging.INFO)
        app.logger.addHandler(file_handler)
        
        app.logger.setLevel(logging.INFO)
        app.logger.info('SEOAnalyzePro startup')
```

### Performance Metrics

```python
# app/core/metrics.py
from prometheus_client import Counter, Histogram, Gauge

# Metrics
analysis_requests = Counter('analysis_requests_total', 'Total analysis requests')
analysis_duration = Histogram('analysis_duration_seconds', 'Analysis duration')
active_analyses = Gauge('active_analyses', 'Currently active analyses')

def track_analysis_metrics(func):
    """Track analysis metrics"""
    @wraps(func)
    def wrapper(*args, **kwargs):
        analysis_requests.inc()
        active_analyses.inc()
        
        start_time = time.time()
        result = func(*args, **kwargs)
        end_time = time.time()
        
        analysis_duration.observe(end_time - start_time)
        active_analyses.dec()
        
        return result
    return wrapper
```

---

*This technical documentation is continuously updated. Last updated: October 2025*


